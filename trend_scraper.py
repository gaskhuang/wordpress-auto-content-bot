"""
trend_scraper.py â€” X (Twitter) èˆ‡ Reddit AEO/GEO è¶¨å‹¢çˆ¬èŸ²

ç”¨é€”ï¼šæ¯é€±å®šæ™‚åŸ·è¡Œï¼Œæ¡é›†é—œæ–¼ AEOã€GEOã€SEO æœ€æ–°å¯«ä½œæŠ€å·§çš„è¨è«–è²¼æ–‡ã€‚
è¼¸å‡ºï¼šraw_trends.json

ä½¿ç”¨æ–¹å¼ï¼š
    python trend_scraper.py                 # å®Œæ•´çˆ¬å–
    python trend_scraper.py --dry-run       # æ¸¬è©¦æ¨¡å¼ï¼Œåƒ…é¡¯ç¤ºæœå°‹çµæœæ•¸é‡
    python trend_scraper.py --source reddit # åƒ…çˆ¬ Reddit
    python trend_scraper.py --source x      # åƒ…çˆ¬ X
"""

import os
import json
import logging
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
    logger.addHandler(handler)

# æœå°‹é—œéµå­—çµ„
SEARCH_QUERIES = [
    "AEO SEO 2026",
    "GEO optimization generative engine",
    "AI search ranking tips",
    "answer engine optimization",
    "generative engine optimization strategy",
]

# Reddit ç›®æ¨™å­ç‰ˆ
REDDIT_SUBREDDITS = ["SEO", "bigseo", "TechSEO", "digital_marketing"]

OUTPUT_FILE = os.path.join(os.path.dirname(__file__), "raw_trends.json")


def scrape_reddit(dry_run=False):
    """å¾ Reddit æ¡é›† AEO/GEO ç›¸é—œè¨è«–"""
    try:
        import praw
    except ImportError:
        logger.error("è«‹å…ˆå®‰è£ praw: pip install praw")
        return []

    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "GEO-Framework-Bot/1.0")

    if not client_id or not client_secret:
        logger.warning("âš ï¸ æœªè¨­å®š REDDIT_CLIENT_ID / REDDIT_CLIENT_SECRETï¼Œè·³é Reddit çˆ¬å–ã€‚")
        return []

    reddit = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent,
    )

    results = []
    one_week_ago = datetime.utcnow() - timedelta(days=7)

    for sub_name in REDDIT_SUBREDDITS:
        try:
            subreddit = reddit.subreddit(sub_name)
            for query in SEARCH_QUERIES:
                logger.info(f"ğŸ” æœå°‹ r/{sub_name}: {query}")
                for post in subreddit.search(query, sort="new", time_filter="week", limit=10):
                    post_time = datetime.utcfromtimestamp(post.created_utc)
                    if post_time < one_week_ago:
                        continue
                    results.append({
                        "source": "reddit",
                        "subreddit": sub_name,
                        "author": str(post.author),
                        "date": post_time.strftime("%Y-%m-%d"),
                        "title": post.title,
                        "text": post.selftext[:1000] if post.selftext else "",
                        "engagement": post.score + post.num_comments,
                        "url": f"https://reddit.com{post.permalink}",
                    })
        except Exception as e:
            logger.error(f"r/{sub_name} çˆ¬å–å¤±æ•—: {e}")

    logger.info(f"ğŸ“Š Reddit å…±æ¡é›† {len(results)} ç¯‡è²¼æ–‡")
    if dry_run:
        logger.info("[DRY RUN] ä¸å¯«å…¥æª”æ¡ˆ")
    return results


def scrape_x(dry_run=False):
    """å¾ X (Twitter) æ¡é›† AEO/GEO ç›¸é—œæ¨æ–‡"""
    try:
        import tweepy
    except ImportError:
        logger.error("è«‹å…ˆå®‰è£ tweepy: pip install tweepy")
        return []

    bearer_token = os.getenv("X_BEARER_TOKEN")
    if not bearer_token:
        logger.warning("âš ï¸ æœªè¨­å®š X_BEARER_TOKENï¼Œè·³é X çˆ¬å–ã€‚")
        return []

    client = tweepy.Client(bearer_token=bearer_token)
    results = []
    one_week_ago = datetime.utcnow() - timedelta(days=7)

    for query in SEARCH_QUERIES:
        try:
            logger.info(f"ğŸ” æœå°‹ X: {query}")
            tweets = client.search_recent_tweets(
                query=f"{query} -is:retweet lang:en",
                max_results=20,
                tweet_fields=["created_at", "public_metrics", "author_id"],
            )
            if tweets.data:
                for tweet in tweets.data:
                    metrics = tweet.public_metrics or {}
                    engagement = metrics.get("like_count", 0) + metrics.get("retweet_count", 0) + metrics.get("reply_count", 0)
                    # åƒ…ä¿ç•™äº’å‹•æ•¸ > 5 çš„æ¨æ–‡ï¼ˆéæ¿¾åƒåœ¾è¨Šæ¯ï¼‰
                    if engagement < 5:
                        continue
                    results.append({
                        "source": "x",
                        "author": str(tweet.author_id),
                        "date": tweet.created_at.strftime("%Y-%m-%d") if tweet.created_at else "",
                        "text": tweet.text,
                        "engagement": engagement,
                        "url": f"https://x.com/i/status/{tweet.id}",
                    })
        except Exception as e:
            logger.error(f"X æœå°‹ '{query}' å¤±æ•—: {e}")

    logger.info(f"ğŸ“Š X å…±æ¡é›† {len(results)} å‰‡æ¨æ–‡")
    if dry_run:
        logger.info("[DRY RUN] ä¸å¯«å…¥æª”æ¡ˆ")
    return results


def main():
    parser = argparse.ArgumentParser(description="AEO/GEO è¶¨å‹¢çˆ¬èŸ²")
    parser.add_argument("--dry-run", action="store_true", help="æ¸¬è©¦æ¨¡å¼ï¼Œä¸å¯«å…¥æª”æ¡ˆ")
    parser.add_argument("--source", choices=["x", "reddit", "all"], default="all", help="é¸æ“‡çˆ¬å–ä¾†æº")
    args = parser.parse_args()

    all_results = []

    if args.source in ("reddit", "all"):
        all_results.extend(scrape_reddit(dry_run=args.dry_run))

    if args.source in ("x", "all"):
        all_results.extend(scrape_x(dry_run=args.dry_run))

    # æŒ‰äº’å‹•é‡æ’åº
    all_results.sort(key=lambda x: x.get("engagement", 0), reverse=True)

    logger.info(f"âœ… ç¸½å…±æ¡é›† {len(all_results)} ç¯‡å…§å®¹")

    if not args.dry_run and all_results:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²å„²å­˜è‡³ {OUTPUT_FILE}")
    elif not all_results:
        logger.warning("âš ï¸ æœªæ¡é›†åˆ°ä»»ä½•å…§å®¹ã€‚è«‹ç¢ºèª API æ†‘è­‰æ˜¯å¦æ­£ç¢ºã€‚")


if __name__ == "__main__":
    main()
