"""
trend_analyzer.py â€” AI è¶¨å‹¢åˆ†æå™¨

ç”¨é€”ï¼šè®€å– raw_trends.jsonï¼Œç”¨ LLM ç¯©é¸å‡ºæœ‰åƒ¹å€¼çš„ AEO/GEO å¯«ä½œæŠ€å·§ã€‚
è¼¸å‡ºï¼šanalyzed_tips.json

ä½¿ç”¨æ–¹å¼ï¼š
    python trend_analyzer.py                        # åˆ†æ raw_trends.json
    python trend_analyzer.py --input custom.json    # åˆ†ææŒ‡å®šæª”æ¡ˆ
    python trend_analyzer.py --dry-run              # åƒ…é¡¯ç¤ºåˆ†æçµæœï¼Œä¸å¯«å…¥
"""

import os
import json
import logging
import argparse
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
    logger.addHandler(handler)

BASE_DIR = os.path.dirname(__file__)
DEFAULT_INPUT = os.path.join(BASE_DIR, "raw_trends.json")
DEFAULT_OUTPUT = os.path.join(BASE_DIR, "analyzed_tips.json")
FRAMEWORK_PATH = os.path.join(BASE_DIR, "geo_framework.md")

# AI åˆ†æ Prompt
ANALYSIS_PROMPT = """ä½ æ˜¯ SEO/AEO/GEO å¯«ä½œè¦ç¯„çš„ç¶­è­·è€…ã€‚ä½ çš„ä»»å‹™æ˜¯å¾ç¤¾ç¾¤åª’é«”è²¼æ–‡ä¸­æå–ã€Œå¯ç›´æ¥æ‡‰ç”¨æ–¼ç¹é«”ä¸­æ–‡éƒ¨è½æ ¼ã€çš„å¯«ä½œæŠ€å·§ã€‚

## ç¾æœ‰è¦ç¯„æ‘˜è¦
{existing_framework}

## æœ¬é€±æ¡é›†çš„ç¤¾ç¾¤è¨è«–
{raw_posts}

## è¼¸å‡ºè¦æ±‚
è«‹ç¯©é¸å‡ºæœ€å¤š 5 æ¢æœ‰åƒ¹å€¼çš„æ–°æŠ€å·§ï¼Œä»¥ JSON é™£åˆ—æ ¼å¼å›å‚³ï¼š
```json
[
  {{
    "tip": "æŠ€å·§æè¿° (ç¹é«”ä¸­æ–‡ï¼Œä¸€å¥è©±)",
    "detail": "è©³ç´°èªªæ˜å¦‚ä½•åœ¨æ–‡ç« ä¸­æ‡‰ç”¨æ­¤æŠ€å·§ (2-3 å¥)",
    "applicable_section": "é©ç”¨çš„éª¨æ¶å€å¡Š (å¦‚ï¼šå°è¨€ã€H2-1ã€FAQ ç­‰)",
    "expected_effect": "é æœŸæ•ˆæœ (å¦‚ï¼šæå‡ AI å¼•ç”¨ç‡ 20%)",
    "source_url": "åŸå§‹è²¼æ–‡ URL",
    "confidence": "high/medium/low"
  }}
]
```

## ç¯©é¸è¦å‰‡
1. æ’é™¤ç´”æ¨éŠ·æˆ–è‡ªæˆ‘å®£å‚³çš„è²¼æ–‡
2. æ’é™¤ç„¡æ•¸æ“šæ”¯æ’çš„å€‹äººçŒœæ¸¬
3. æ’é™¤åƒ…é©ç”¨æ–¼è‹±æ–‡çš„æŠ€å·§
4. å„ªå…ˆä¿ç•™æœ‰å…·é«”æ•¸æ“šæˆ–æ¡ˆä¾‹æ”¯æŒçš„æŠ€å·§
5. èˆ‡ç¾æœ‰è¦ç¯„é‡è¤‡çš„æŠ€å·§ä¸è¦åˆ—å…¥
"""


def load_existing_framework():
    """è®€å–ç¾æœ‰çš„ geo_framework.md ä½œç‚ºæ¯”å°åŸºæº–"""
    try:
        with open(FRAMEWORK_PATH, "r", encoding="utf-8") as f:
            content = f.read()
        # åªå–ã€Œæ ¸å¿ƒéª¨æ¶ã€å€å¡Šä½œç‚ºæ‘˜è¦ (ç¯€çœ Token)
        if "## äºŒã€é€²éšæŠ€å·§åº«" in content:
            return content.split("## äºŒã€é€²éšæŠ€å·§åº«")[0]
        return content[:2000]
    except FileNotFoundError:
        logger.warning(f"æ‰¾ä¸åˆ° {FRAMEWORK_PATH}ï¼Œå°‡ä½¿ç”¨é è¨­è¦ç¯„")
        return "(å°šç„¡ç¾æœ‰è¦ç¯„)"


def analyze_with_openai(raw_posts, existing_framework):
    """ä½¿ç”¨ OpenAI API é€²è¡Œè¶¨å‹¢åˆ†æ"""
    try:
        import openai
    except ImportError:
        logger.error("è«‹å…ˆå®‰è£ openai: pip install openai")
        return []

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.warning("âš ï¸ æœªè¨­å®š OPENAI_API_KEYï¼Œå˜—è©¦ä½¿ç”¨æœ¬åœ°åˆ†ææ¨¡å¼")
        return analyze_locally(raw_posts)

    client = openai.OpenAI(api_key=api_key)

    # æº–å‚™è²¼æ–‡æ‘˜è¦ (é™åˆ¶ Token æ¶ˆè€—)
    posts_text = ""
    for i, post in enumerate(raw_posts[:30]):  # æœ€å¤š 30 ç¯‡
        posts_text += f"\n---\n[{i+1}] ä¾†æº: {post.get('source')} | äº’å‹•: {post.get('engagement', 0)}\n"
        posts_text += f"å…§å®¹: {post.get('text', '')[:300]}\n"
        posts_text += f"URL: {post.get('url', '')}\n"

    prompt = ANALYSIS_PROMPT.format(
        existing_framework=existing_framework[:1500],
        raw_posts=posts_text
    )

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            response_format={"type": "json_object"},
        )
        result_text = response.choices[0].message.content
        result = json.loads(result_text)
        # æ”¯æ´ {"tips": [...]} æˆ–ç›´æ¥ [...]
        if isinstance(result, dict) and "tips" in result:
            return result["tips"]
        elif isinstance(result, list):
            return result
        else:
            logger.warning(f"AI å›å‚³æ ¼å¼ç•°å¸¸: {type(result)}")
            return []
    except Exception as e:
        logger.error(f"OpenAI API å‘¼å«å¤±æ•—: {e}")
        return []


def analyze_locally(raw_posts):
    """æœ¬åœ°åˆ†ææ¨¡å¼ (ç„¡éœ€ API)ï¼šæŒ‰äº’å‹•é‡æ’åºï¼Œæå–é«˜äº’å‹•è²¼æ–‡"""
    logger.info("ğŸ“ ä½¿ç”¨æœ¬åœ°åˆ†ææ¨¡å¼ (æŒ‰äº’å‹•é‡æ’åº)")
    tips = []
    seen_urls = set()
    for post in sorted(raw_posts, key=lambda x: x.get("engagement", 0), reverse=True)[:5]:
        url = post.get("url", "")
        if url in seen_urls:
            continue
        seen_urls.add(url)
        tips.append({
            "tip": post.get("title", post.get("text", "")[:80]),
            "detail": post.get("text", "")[:200],
            "applicable_section": "å¾…äººå·¥åˆ†é¡",
            "expected_effect": f"äº’å‹•é‡: {post.get('engagement', 0)}",
            "source_url": url,
            "confidence": "medium" if post.get("engagement", 0) > 50 else "low",
        })
    return tips


def main():
    parser = argparse.ArgumentParser(description="AEO/GEO è¶¨å‹¢åˆ†æå™¨")
    parser.add_argument("--input", default=DEFAULT_INPUT, help="è¼¸å…¥çš„åŸå§‹è¶¨å‹¢ JSON æª”æ¡ˆ")
    parser.add_argument("--output", default=DEFAULT_OUTPUT, help="è¼¸å‡ºçš„åˆ†æçµæœ JSON æª”æ¡ˆ")
    parser.add_argument("--dry-run", action="store_true", help="åƒ…é¡¯ç¤ºçµæœï¼Œä¸å¯«å…¥æª”æ¡ˆ")
    args = parser.parse_args()

    # è®€å–åŸå§‹è³‡æ–™
    if not os.path.exists(args.input):
        logger.error(f"âŒ æ‰¾ä¸åˆ° {args.input}ï¼Œè«‹å…ˆåŸ·è¡Œ trend_scraper.py")
        return

    with open(args.input, "r", encoding="utf-8") as f:
        raw_posts = json.load(f)

    logger.info(f"ğŸ“– è®€å–äº† {len(raw_posts)} ç¯‡åŸå§‹è²¼æ–‡")

    # è®€å–ç¾æœ‰è¦ç¯„
    existing_framework = load_existing_framework()

    # AI åˆ†æ
    tips = analyze_with_openai(raw_posts, existing_framework)

    logger.info(f"âœ… åˆ†æå®Œæˆï¼Œç”¢å‡º {len(tips)} æ¢æ–°æŠ€å·§")

    for i, tip in enumerate(tips):
        logger.info(f"  [{i+1}] {tip.get('tip', '')} (ä¿¡å¿ƒåº¦: {tip.get('confidence', 'unknown')})")

    if not args.dry_run and tips:
        output_data = {
            "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source_file": args.input,
            "total_raw_posts": len(raw_posts),
            "tips": tips,
        }
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²å„²å­˜è‡³ {args.output}")


if __name__ == "__main__":
    main()
