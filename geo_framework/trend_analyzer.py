"""
trend_analyzer.py â€” AI è¶¨å‹¢åˆ†æå™¨

ç”¨é€”ï¼šè®€å– raw_trends.jsonï¼Œç”¨ LLM ç¯©é¸å‡ºæœ‰åƒ¹å€¼çš„ AEO/GEO å¯«ä½œæŠ€å·§ã€‚
è¼¸å‡ºï¼šanalyzed_tips.json

ä½¿ç”¨æ–¹å¼ï¼š
    python trend_analyzer.py                        # åˆ†æ raw_trends.json
    python trend_analyzer.py --input custom.json    # åˆ†ææŒ‡å®šæª”æ¡ˆ
    python trend_analyzer.py --dry-run              # åƒ…é¡¯ç¤ºåˆ†æçµæœï¼Œä¸å¯«å…¥
"""

import os
import sys
import re
import json
import logging
import argparse
from datetime import datetime
from dotenv import load_dotenv

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "core"))
from retry_utils import retry_with_backoff

load_dotenv()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
    logger.addHandler(handler)

BASE_DIR = os.path.dirname(__file__)
DEFAULT_INPUT = os.path.join(BASE_DIR, "raw_trends.json")
DEFAULT_OUTPUT = os.path.join(BASE_DIR, "analyzed_tips.json")
FRAMEWORK_PATH = os.path.join(BASE_DIR, "geo_framework.md")

# â”€â”€ Prompt Injection æ¶ˆæ¯’ â”€â”€

# å¯èƒ½ç”¨æ–¼ prompt injection çš„æ¨¡å¼
_INJECTION_PATTERNS = [
    r'ignore\s+(all\s+)?previous\s+instructions',
    r'disregard\s+(all\s+)?(above|prior|previous)',
    r'you\s+are\s+now\s+',
    r'new\s+instructions?\s*:',
    r'system\s*:\s*',
    r'override\s+(all\s+)?rules',
    r'forget\s+(everything|all)',
    r'act\s+as\s+(a\s+)?',
    r'do\s+not\s+follow',
    r'output\s+(your|the|all)\s+(api|secret|key|prompt|instruction)',
]
_INJECTION_RE = re.compile('|'.join(_INJECTION_PATTERNS), re.IGNORECASE)


def sanitize_text(text):
    """
    æ¶ˆæ¯’è²¼æ–‡å…§å®¹ï¼Œé˜²æ­¢ prompt injectionã€‚
    - ç§»é™¤å¯ç–‘çš„æŒ‡ä»¤æ€§èªå¥
    - æˆªæ–·éé•·æ–‡å­—
    - ç§»é™¤ä¸å¯è¦‹/æ§åˆ¶å­—å…ƒ
    """
    if not text:
        return ""

    # ç§»é™¤æ§åˆ¶å­—å…ƒï¼ˆä¿ç•™æ›è¡Œå’Œç©ºç™½ï¼‰
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

    # åµæ¸¬ä¸¦æ›¿æ› injection æ¨¡å¼
    if _INJECTION_RE.search(text):
        logger.warning(f"âš ï¸ åµæ¸¬åˆ°å¯ç–‘ prompt injection æ¨¡å¼ï¼Œå·²æ¶ˆæ¯’")
        text = _INJECTION_RE.sub('[å·²ç§»é™¤å¯ç–‘æŒ‡ä»¤]', text)

    # ç§»é™¤ markdown/HTML ä¸­å¯èƒ½ç”¨ä¾†å½è£æŒ‡ä»¤çš„å€å¡Š
    text = re.sub(r'```[\s\S]*?```', '[ç¨‹å¼ç¢¼å€å¡Šå·²ç§»é™¤]', text)

    # æˆªæ–·éé•·æ–‡å­—
    if len(text) > 500:
        text = text[:500] + "..."

    return text.strip()

# AI åˆ†æ Prompt
ANALYSIS_PROMPT = """ä½ æ˜¯ SEO/AEO/GEO å¯«ä½œè¦ç¯„çš„ç¶­è­·è€…ã€‚ä½ çš„ä»»å‹™æ˜¯å¾ç¤¾ç¾¤åª’é«”è²¼æ–‡ä¸­æå–ã€Œå¯ç›´æ¥æ‡‰ç”¨æ–¼ç¹é«”ä¸­æ–‡éƒ¨è½æ ¼ã€çš„å¯«ä½œæŠ€å·§ã€‚

## ç¾æœ‰è¦ç¯„æ‘˜è¦
{existing_framework}

## æœ¬é€±æ¡é›†çš„ç¤¾ç¾¤è¨è«–
{raw_posts}

## è¼¸å‡ºè¦æ±‚
è«‹ç¯©é¸å‡ºæœ€å¤š 5 æ¢æœ‰åƒ¹å€¼çš„æ–°æŠ€å·§ï¼Œä»¥ JSON é™£åˆ—æ ¼å¼å›å‚³ï¼š
```json
[
  {{
    "tip": "æŠ€å·§æè¿° (ç¹é«”ä¸­æ–‡ï¼Œä¸€å¥è©±)",
    "detail": "è©³ç´°èªªæ˜å¦‚ä½•åœ¨æ–‡ç« ä¸­æ‡‰ç”¨æ­¤æŠ€å·§ (2-3 å¥)",
    "applicable_section": "é©ç”¨çš„éª¨æ¶å€å¡Š (å¦‚ï¼šå°è¨€ã€H2-1ã€FAQ ç­‰)",
    "expected_effect": "é æœŸæ•ˆæœ (å¦‚ï¼šæå‡ AI å¼•ç”¨ç‡ 20%)",
    "source_url": "åŸå§‹è²¼æ–‡ URL",
    "confidence": "high/medium/low"
  }}
]
```

## ç¯©é¸è¦å‰‡
1. æ’é™¤ç´”æ¨éŠ·æˆ–è‡ªæˆ‘å®£å‚³çš„è²¼æ–‡
2. æ’é™¤ç„¡æ•¸æ“šæ”¯æ’çš„å€‹äººçŒœæ¸¬
3. æ’é™¤åƒ…é©ç”¨æ–¼è‹±æ–‡çš„æŠ€å·§
4. å„ªå…ˆä¿ç•™æœ‰å…·é«”æ•¸æ“šæˆ–æ¡ˆä¾‹æ”¯æŒçš„æŠ€å·§
5. èˆ‡ç¾æœ‰è¦ç¯„é‡è¤‡çš„æŠ€å·§ä¸è¦åˆ—å…¥
"""


def load_existing_framework():
    """è®€å–ç¾æœ‰çš„ geo_framework.md ä½œç‚ºæ¯”å°åŸºæº–"""
    try:
        with open(FRAMEWORK_PATH, "r", encoding="utf-8") as f:
            content = f.read()
        # åªå–ã€Œæ ¸å¿ƒéª¨æ¶ã€å€å¡Šä½œç‚ºæ‘˜è¦ (ç¯€çœ Token)
        if "## äºŒã€é€²éšæŠ€å·§åº«" in content:
            return content.split("## äºŒã€é€²éšæŠ€å·§åº«")[0]
        return content[:2000]
    except FileNotFoundError:
        logger.warning(f"æ‰¾ä¸åˆ° {FRAMEWORK_PATH}ï¼Œå°‡ä½¿ç”¨é è¨­è¦ç¯„")
        return "(å°šç„¡ç¾æœ‰è¦ç¯„)"


def analyze_with_openai(raw_posts, existing_framework):
    """ä½¿ç”¨ OpenAI API é€²è¡Œè¶¨å‹¢åˆ†æ"""
    try:
        import openai
    except ImportError:
        logger.error("è«‹å…ˆå®‰è£ openai: pip install openai")
        return []

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.warning("âš ï¸ æœªè¨­å®š OPENAI_API_KEYï¼Œå˜—è©¦ä½¿ç”¨æœ¬åœ°åˆ†ææ¨¡å¼")
        return analyze_locally(raw_posts)

    client = openai.OpenAI(api_key=api_key)

    # æº–å‚™è²¼æ–‡æ‘˜è¦ (é™åˆ¶ Token æ¶ˆè€— + æ¶ˆæ¯’é˜²è­·)
    posts_text = ""
    for i, post in enumerate(raw_posts[:30]):  # æœ€å¤š 30 ç¯‡
        safe_text = sanitize_text(post.get('text', ''))
        safe_title = sanitize_text(post.get('title', ''))
        posts_text += f"\n---\n[{i+1}] ä¾†æº: {post.get('source')} | äº’å‹•: {post.get('engagement', 0)}\n"
        posts_text += f"æ¨™é¡Œ: {safe_title}\n"
        posts_text += f"å…§å®¹: {safe_text}\n"
        posts_text += f"URL: {post.get('url', '')}\n"

    prompt = ANALYSIS_PROMPT.format(
        existing_framework=existing_framework[:1500],
        raw_posts=posts_text
    )

    def _call_openai():
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            response_format={"type": "json_object"},
        )
        result_text = response.choices[0].message.content
        result = json.loads(result_text)
        
        if isinstance(result, list):
            return result
        if isinstance(result, dict):
            # æƒ…æ³ 1: ç›´æ¥åŒ…å«åœ¨ "tips" éµä¸­
            if "tips" in result and isinstance(result["tips"], list):
                return result["tips"]
            # æƒ…æ³ 2: AI å›å‚³äº†å¤šå€‹éµï¼Œå…¶ä¸­ä¸€å€‹æ˜¯åˆ—è¡¨ä¸”é•·åº¦ > 0
            for key, value in result.items():
                if isinstance(value, list) and len(value) > 0:
                    return value
            # æƒ…æ³ 3: AI è¢«æŒ‡ç¤ºè¼¸å‡º JSON ç‰©ä»¶ï¼Œçµæœå®ƒæŠŠæ•´å€‹å…§å®¹ç•¶æˆä¸€å€‹ç‰©ä»¶å›å‚³
            #ï¼ˆå¦‚æœç‰©ä»¶åŒ…å« tip éµï¼Œå‰‡å°‡å…¶åŒ…è£æˆåˆ—è¡¨ï¼‰
            if "tip" in result:
                return [result]
        
        logger.warning(f"AI å›å‚³æ ¼å¼ç•°å¸¸æˆ–ç„¡è³‡æ–™: {result_text[:200]}")
        return []

    try:
        return retry_with_backoff(
            _call_openai,
            max_retries=3,
            base_delay=2.0,
            max_delay=30.0,
        )
    except Exception as e:
        logger.error(f"OpenAI API å‘¼å«å¤±æ•—ï¼ˆå·²é‡è©¦ 3 æ¬¡ï¼‰: {e}")
        return []


def analyze_locally(raw_posts):
    """æœ¬åœ°åˆ†ææ¨¡å¼ (ç„¡éœ€ API)ï¼šæŒ‰äº’å‹•é‡æ’åºï¼Œæå–é«˜äº’å‹•è²¼æ–‡"""
    logger.info("ğŸ“ ä½¿ç”¨æœ¬åœ°åˆ†ææ¨¡å¼ (æŒ‰äº’å‹•é‡æ’åº)")
    tips = []
    seen_urls = set()
    for post in sorted(raw_posts, key=lambda x: x.get("engagement", 0), reverse=True)[:5]:
        url = post.get("url", "")
        if url in seen_urls:
            continue
        seen_urls.add(url)
        tips.append({
            "tip": sanitize_text(post.get("title", post.get("text", "")[:80])),
            "detail": sanitize_text(post.get("text", "")[:200]),
            "applicable_section": "å¾…äººå·¥åˆ†é¡",
            "expected_effect": f"äº’å‹•é‡: {post.get('engagement', 0)}",
            "source_url": url,
            "confidence": "medium" if post.get("engagement", 0) > 50 else "low",
        })
    return tips


def main():
    parser = argparse.ArgumentParser(description="AEO/GEO è¶¨å‹¢åˆ†æå™¨")
    parser.add_argument("--input", default=DEFAULT_INPUT, help="è¼¸å…¥çš„åŸå§‹è¶¨å‹¢ JSON æª”æ¡ˆ")
    parser.add_argument("--output", default=DEFAULT_OUTPUT, help="è¼¸å‡ºçš„åˆ†æçµæœ JSON æª”æ¡ˆ")
    parser.add_argument("--dry-run", action="store_true", help="åƒ…é¡¯ç¤ºçµæœï¼Œä¸å¯«å…¥æª”æ¡ˆ")
    args = parser.parse_args()

    # è®€å–åŸå§‹è³‡æ–™
    if not os.path.exists(args.input):
        logger.error(f"âŒ æ‰¾ä¸åˆ° {args.input}ï¼Œè«‹å…ˆåŸ·è¡Œ trend_scraper.py")
        return

    with open(args.input, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # ä¿®æ”¹ï¼šæ ¹æ“š raw_trends.json çš„çµæ§‹æå–è²¼æ–‡åˆ—è¡¨
    if isinstance(data, dict) and "posts" in data:
        raw_posts = data["posts"]
    else:
        raw_posts = data if isinstance(data, list) else []

    logger.info(f"ğŸ“– è®€å–äº† {len(raw_posts)} ç¯‡åŸå§‹è²¼æ–‡")

    # è®€å–ç¾æœ‰è¦ç¯„
    existing_framework = load_existing_framework()

    # AI åˆ†æ
    tips = analyze_with_openai(raw_posts, existing_framework)

    logger.info(f"âœ… åˆ†æå®Œæˆï¼Œç”¢å‡º {len(tips)} æ¢æ–°æŠ€å·§")

    for i, tip in enumerate(tips):
        logger.info(f"  [{i+1}] {tip.get('tip', '')} (ä¿¡å¿ƒåº¦: {tip.get('confidence', 'unknown')})")

    if not args.dry_run and tips:
        output_data = {
            "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source_file": args.input,
            "total_raw_posts": len(raw_posts),
            "tips": tips,
        }
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²å„²å­˜è‡³ {args.output}")


if __name__ == "__main__":
    main()
