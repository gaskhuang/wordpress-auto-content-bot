"""
trend_scraper.py â€” AEO/GEO è¶¨å‹¢çˆ¬èŸ² (Zero-API Mode + API Fallback)

ç”¨é€”ï¼šæ¯é€±å®šæ™‚åŸ·è¡Œï¼Œæ¡é›†é—œæ–¼ AEOã€GEOã€SEO æœ€æ–°å¯«ä½œæŠ€å·§çš„è¨è«–è²¼æ–‡ã€‚
è¼¸å‡ºï¼šraw_trends.json

V2 å‡ç´šèªªæ˜ï¼š
  - å„ªå…ˆä½¿ç”¨é›¶ API æ¨¡å¼ï¼ˆrequests ç›´æ¥è®€å– Reddit å…¬é–‹ JSON + Google æœå°‹ï¼‰
  - è‹¥æœ‰è¨­å®š API keyï¼Œå‰‡è‡ªå‹•å‡ç´šç‚º API æ¨¡å¼ï¼ˆpraw / tweepyï¼‰
  - å®Œå…¨ä¸éœ€è¨­å®šä»»ä½• API æ†‘è­‰å³å¯é‹ä½œ

ä½¿ç”¨æ–¹å¼ï¼š
    python trend_scraper.py                 # å®Œæ•´çˆ¬å–ï¼ˆè‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å¼ï¼‰
    python trend_scraper.py --dry-run       # æ¸¬è©¦æ¨¡å¼ï¼Œåƒ…é¡¯ç¤ºæœå°‹çµæœæ•¸é‡
    python trend_scraper.py --source reddit # åƒ…çˆ¬ Reddit
    python trend_scraper.py --source web    # åƒ…çˆ¬ Google æœå°‹
    python trend_scraper.py --mode api      # å¼·åˆ¶ä½¿ç”¨ API æ¨¡å¼
"""

import os
import sys
import json
import logging
import argparse
import re
import time
from datetime import datetime, timedelta
from dotenv import load_dotenv

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "core"))
from retry_utils import retry_with_backoff

load_dotenv()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S'))
    logger.addHandler(handler)

# æœå°‹é—œéµå­—çµ„
SEARCH_QUERIES = [
    "AEO SEO 2026",
    "GEO optimization generative engine",
    "AI search ranking tips",
    "answer engine optimization",
    "generative engine optimization strategy",
]

# Reddit ç›®æ¨™å­ç‰ˆ
REDDIT_SUBREDDITS = ["SEO", "bigseo", "TechSEO", "digital_marketing"]

OUTPUT_FILE = os.path.join(os.path.dirname(__file__), "raw_trends.json")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  é›¶ API æ¨¡å¼ï¼šReddit å…¬é–‹ JSON + requests
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape_reddit_zero_api(dry_run=False):
    """é›¶ API æ¨¡å¼ï¼šé€é Reddit å…¬é–‹ JSON ç«¯é»æ¡é›† AEO/GEO è¨è«–"""
    import requests

    results = []
    headers = {
        "User-Agent": "GEO-Framework-Bot/2.0 (Zero-API Trend Scraper)"
    }

    for sub_name in REDDIT_SUBREDDITS:
        for query in SEARCH_QUERIES:
            url = f"https://www.reddit.com/r/{sub_name}/search.json"
            params = {
                "q": query,
                "sort": "new",
                "t": "month",
                "limit": 10,
                "restrict_sr": "true",
            }
            logger.info(f"ğŸ” [Zero-API] æœå°‹ r/{sub_name}: {query}")

            try:
                def _fetch_reddit():
                    r = requests.get(url, headers=headers, params=params, timeout=10)
                    r.raise_for_status()
                    return r.json()

                try:
                    data = retry_with_backoff(
                        _fetch_reddit,
                        max_retries=3,
                        base_delay=2.0,
                        retryable_exceptions=(requests.exceptions.RequestException,),
                    )
                except requests.exceptions.RequestException:
                    logger.warning(f"r/{sub_name} æœå°‹ '{query}' é‡è©¦å¾Œä»å¤±æ•—ï¼Œè·³é")
                    continue
                posts = data.get("data", {}).get("children", [])

                for post_wrapper in posts:
                    post = post_wrapper.get("data", {})
                    # éæ¿¾ä¸ç›¸é—œå…§å®¹ï¼ˆæ¨™é¡Œæˆ–å…§æ–‡ä¸­éœ€åŒ…å«é—œéµå­—ï¼‰
                    title = post.get("title", "")
                    selftext = post.get("selftext", "")
                    combined = (title + " " + selftext).lower()

                    # è‡³å°‘éœ€åŒ…å«ä¸€å€‹æ ¸å¿ƒé—œéµå­—
                    keywords = ["aeo", "geo", "generative engine", "answer engine",
                                "ai search", "ai optimization", "llm", "seo"]
                    if not any(kw in combined for kw in keywords):
                        continue

                    engagement = post.get("score", 0) + post.get("num_comments", 0)
                    created_utc = post.get("created_utc", 0)
                    post_date = datetime.utcfromtimestamp(created_utc).strftime("%Y-%m-%d") if created_utc else ""

                    results.append({
                        "source": "reddit",
                        "subreddit": sub_name,
                        "author": str(post.get("author", "unknown")),
                        "date": post_date,
                        "title": title,
                        "text": selftext[:1000] if selftext else "",
                        "engagement": engagement,
                        "url": f"https://reddit.com{post.get('permalink', '')}",
                    })

                # ç¦®è²Œæ€§å»¶é²ï¼Œé¿å…è¢« Reddit å°é–
                time.sleep(1)

            except Exception as e:
                logger.error(f"r/{sub_name} æœå°‹ '{query}' å¤±æ•—: {e}")

    # å»é‡ï¼ˆä¾ URLï¼‰
    seen_urls = set()
    unique_results = []
    for r in results:
        if r["url"] not in seen_urls:
            seen_urls.add(r["url"])
            unique_results.append(r)

    logger.info(f"ğŸ“Š [Zero-API] Reddit å…±æ¡é›† {len(unique_results)} ç¯‡ç›¸é—œè²¼æ–‡")
    if dry_run:
        logger.info("[DRY RUN] ä¸å¯«å…¥æª”æ¡ˆ")
    return unique_results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  é›¶ API æ¨¡å¼ï¼šGoogle æœå°‹çµæœæ‘˜è¦
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape_web_search(dry_run=False):
    """
    é›¶ API æ¨¡å¼ï¼šé€é requests æŠ“å– SEO/GEO æ–‡ç« ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•¸è¨­è¨ˆç‚ºæ­é…å¤–éƒ¨å·¥å…· (search_web / read_url_content) ä½¿ç”¨ï¼Œ
    æˆ–ç”±ä½¿ç”¨è€…æä¾› URL æ¸…å–®ã€‚

    å–®ç¨åŸ·è¡Œæ™‚ï¼Œæœƒå˜—è©¦è®€å–å·²çŸ¥çš„é«˜è³ªé‡ SEO ç¶²ç«™ RSS/æœ€æ–°æ–‡ç« é é¢ã€‚
    """
    import requests

    # å·²çŸ¥é«˜å“è³ª SEO ä¾†æºçš„ RSS / JSON feeds
    KNOWN_SOURCES = [
        {
            "name": "Search Engine Journal",
            "url": "https://www.searchenginejournal.com/category/seo/feed/",
            "type": "rss"
        },
        {
            "name": "Search Engine Land",
            "url": "https://searchengineland.com/feed",
            "type": "rss"
        },
    ]

    results = []
    headers = {
        "User-Agent": "GEO-Framework-Bot/2.0 (Zero-API Trend Scraper)"
    }

    for source in KNOWN_SOURCES:
        try:
            logger.info(f"ğŸ” [Zero-API] è®€å– {source['name']} RSS...")

            def _fetch_rss():
                r = requests.get(source["url"], headers=headers, timeout=10)
                r.raise_for_status()
                return r.text

            try:
                content = retry_with_backoff(
                    _fetch_rss,
                    max_retries=3,
                    base_delay=2.0,
                    retryable_exceptions=(requests.exceptions.RequestException,),
                )
            except requests.exceptions.RequestException:
                logger.warning(f"{source['name']} é‡è©¦å¾Œä»å¤±æ•—ï¼Œè·³é")
                continue
            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå– <item> ä¸­çš„ <title> å’Œ <link>
            items = re.findall(
                r'<item>.*?<title><!\[CDATA\[(.*?)\]\]></title>.*?<link>(.*?)</link>.*?</item>',
                content,
                re.DOTALL
            )
            if not items:
                # å˜—è©¦ä¸å¸¶ CDATA çš„æ ¼å¼
                items = re.findall(
                    r'<item>.*?<title>(.*?)</title>.*?<link>(.*?)</link>.*?</item>',
                    content,
                    re.DOTALL
                )

            geo_keywords = ["aeo", "geo", "generative engine", "answer engine",
                            "ai search", "ai overview", "llm optimization",
                            "structured data", "schema markup", "e-e-a-t"]

            for title, link in items[:30]:
                title_clean = title.strip()
                if any(kw in title_clean.lower() for kw in geo_keywords):
                    results.append({
                        "source": "web",
                        "site": source["name"],
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "title": title_clean,
                        "text": "",
                        "engagement": 0,
                        "url": link.strip(),
                    })

        except Exception as e:
            logger.error(f"{source['name']} è®€å–å¤±æ•—: {e}")

    logger.info(f"ğŸ“Š [Zero-API] Web å…±æ¡é›† {len(results)} ç¯‡ç›¸é—œæ–‡ç« ")
    if dry_run:
        logger.info("[DRY RUN] ä¸å¯«å…¥æª”æ¡ˆ")
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  API æ¨¡å¼ (Fallback)ï¼šéœ€è¦ API Key
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape_reddit_api(dry_run=False):
    """API æ¨¡å¼ï¼šä½¿ç”¨ praw æ¡é›† Redditï¼ˆéœ€ REDDIT_CLIENT_ID / SECRETï¼‰"""
    try:
        import praw
    except ImportError:
        logger.warning("praw æœªå®‰è£ï¼Œè·³é API æ¨¡å¼ Reddit çˆ¬å–")
        return []

    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT", "GEO-Framework-Bot/2.0")

    if not client_id or not client_secret:
        logger.info("æœªè¨­å®š Reddit API æ†‘è­‰ï¼Œè·³é API æ¨¡å¼")
        return []

    reddit = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent,
    )

    results = []
    one_week_ago = datetime.utcnow() - timedelta(days=7)

    for sub_name in REDDIT_SUBREDDITS:
        try:
            subreddit = reddit.subreddit(sub_name)
            for query in SEARCH_QUERIES:
                logger.info(f"ğŸ” [API] æœå°‹ r/{sub_name}: {query}")
                for post in subreddit.search(query, sort="new", time_filter="week", limit=10):
                    post_time = datetime.utcfromtimestamp(post.created_utc)
                    if post_time < one_week_ago:
                        continue
                    results.append({
                        "source": "reddit",
                        "subreddit": sub_name,
                        "author": str(post.author),
                        "date": post_time.strftime("%Y-%m-%d"),
                        "title": post.title,
                        "text": post.selftext[:1000] if post.selftext else "",
                        "engagement": post.score + post.num_comments,
                        "url": f"https://reddit.com{post.permalink}",
                    })
        except Exception as e:
            logger.error(f"r/{sub_name} çˆ¬å–å¤±æ•—: {e}")

    logger.info(f"ğŸ“Š [API] Reddit å…±æ¡é›† {len(results)} ç¯‡è²¼æ–‡")
    return results


def scrape_x_api(dry_run=False):
    """API æ¨¡å¼ï¼šä½¿ç”¨ tweepy æ¡é›† Xï¼ˆéœ€ X_BEARER_TOKENï¼‰"""
    try:
        import tweepy
    except ImportError:
        logger.warning("tweepy æœªå®‰è£ï¼Œè·³é X çˆ¬å–")
        return []

    bearer_token = os.getenv("X_BEARER_TOKEN")
    if not bearer_token:
        logger.info("æœªè¨­å®š X_BEARER_TOKENï¼Œè·³é X çˆ¬å–")
        return []

    client = tweepy.Client(bearer_token=bearer_token)
    results = []

    for query in SEARCH_QUERIES:
        try:
            logger.info(f"ğŸ” [API] æœå°‹ X: {query}")
            tweets = client.search_recent_tweets(
                query=f"{query} -is:retweet lang:en",
                max_results=20,
                tweet_fields=["created_at", "public_metrics", "author_id"],
            )
            if tweets.data:
                for tweet in tweets.data:
                    metrics = tweet.public_metrics or {}
                    engagement = (metrics.get("like_count", 0) +
                                  metrics.get("retweet_count", 0) +
                                  metrics.get("reply_count", 0))
                    if engagement < 5:
                        continue
                    results.append({
                        "source": "x",
                        "author": str(tweet.author_id),
                        "date": tweet.created_at.strftime("%Y-%m-%d") if tweet.created_at else "",
                        "text": tweet.text,
                        "engagement": engagement,
                        "url": f"https://x.com/i/status/{tweet.id}",
                    })
        except Exception as e:
            logger.error(f"X æœå°‹ '{query}' å¤±æ•—: {e}")

    logger.info(f"ğŸ“Š [API] X å…±æ¡é›† {len(results)} å‰‡æ¨æ–‡")
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ä¸»ç¨‹å¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="AEO/GEO è¶¨å‹¢çˆ¬èŸ² (Zero-API + API Fallback)")
    parser.add_argument("--dry-run", action="store_true", help="æ¸¬è©¦æ¨¡å¼ï¼Œä¸å¯«å…¥æª”æ¡ˆ")
    parser.add_argument("--source", choices=["reddit", "web", "x", "all"], default="all",
                        help="é¸æ“‡çˆ¬å–ä¾†æº")
    parser.add_argument("--mode", choices=["auto", "api", "zero"], default="auto",
                        help="auto=è‡ªå‹•åµæ¸¬, api=å¼·åˆ¶ API, zero=å¼·åˆ¶é›¶ API")
    args = parser.parse_args()

    all_results = []

    # æ±ºå®šæ˜¯å¦ä½¿ç”¨ API æ¨¡å¼
    has_reddit_api = bool(os.getenv("REDDIT_CLIENT_ID") and os.getenv("REDDIT_CLIENT_SECRET"))
    has_x_api = bool(os.getenv("X_BEARER_TOKEN"))

    use_api = (args.mode == "api") or (args.mode == "auto" and (has_reddit_api or has_x_api))
    use_zero = (args.mode == "zero") or (args.mode == "auto")

    if use_api:
        logger.info("ğŸ”‘ åµæ¸¬åˆ° API æ†‘è­‰ï¼Œä½¿ç”¨ API æ¨¡å¼")
    if use_zero:
        logger.info("ğŸŒ å•Ÿç”¨é›¶ API æ¨¡å¼")

    # Reddit çˆ¬å–
    if args.source in ("reddit", "all"):
        if use_api and has_reddit_api and args.mode != "zero":
            all_results.extend(scrape_reddit_api(dry_run=args.dry_run))
        if use_zero:
            all_results.extend(scrape_reddit_zero_api(dry_run=args.dry_run))

    # Web æœå°‹çˆ¬å–
    if args.source in ("web", "all"):
        if use_zero:
            all_results.extend(scrape_web_search(dry_run=args.dry_run))

    # X çˆ¬å–ï¼ˆåƒ… API æ¨¡å¼ï¼‰
    if args.source in ("x", "all"):
        if use_api and has_x_api:
            all_results.extend(scrape_x_api(dry_run=args.dry_run))

    # å»é‡ + æŒ‰äº’å‹•é‡æ’åº
    seen_urls = set()
    unique_results = []
    for r in all_results:
        if r["url"] not in seen_urls:
            seen_urls.add(r["url"])
            unique_results.append(r)

    unique_results.sort(key=lambda x: x.get("engagement", 0), reverse=True)

    logger.info(f"âœ… ç¸½å…±æ¡é›† {len(unique_results)} ç¯‡å…§å®¹ï¼ˆå»é‡å¾Œï¼‰")

    if not args.dry_run and unique_results:
        output = {
            "scrape_date": datetime.now().strftime("%Y-%m-%d"),
            "scrape_method": "Zero-API" if use_zero else "API",
            "total_posts": len(unique_results),
            "posts": unique_results,
        }
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²å„²å­˜è‡³ {OUTPUT_FILE}")
    elif not unique_results:
        logger.warning("âš ï¸ æœªæ¡é›†åˆ°ä»»ä½•å…§å®¹ã€‚")


if __name__ == "__main__":
    main()
